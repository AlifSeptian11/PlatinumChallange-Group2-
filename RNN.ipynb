{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "167298b9-d01b-4121-aad6-833f863cc1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Reza Fakhrurrozi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, SimpleRNN, Activation, Flatten\n",
    "from tensorflow.keras import optimizers, callbacks\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c68c1e3-1a42-4b7f-b932-610c65d0cc9f",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97194382-19c3-4f2a-8ffa-b7aa48993f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>warung ini dimiliki oleh pengusaha pabrik tahu...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mohon ulama lurus dan k212 mmbri hujjah partai...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lokasi strategis di jalan sumatera bandung . t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>betapa bahagia nya diri ini saat unboxing pake...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>duh . jadi mahasiswa jangan sombong dong . kas...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet     Label\n",
       "0  warung ini dimiliki oleh pengusaha pabrik tahu...  positive\n",
       "1  mohon ulama lurus dan k212 mmbri hujjah partai...   neutral\n",
       "2  lokasi strategis di jalan sumatera bandung . t...  positive\n",
       "3  betapa bahagia nya diri ini saat unboxing pake...  positive\n",
       "4  duh . jadi mahasiswa jangan sombong dong . kas...  negative"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dataset\n",
    "df = pd.read_csv('DATA/train_preprocess.tsv.txt', sep='\\t', names=['Tweet','Label'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51c370d6-f964-44bc-b2e0-116458f46c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "positive    6416\n",
       "negative    3436\n",
       "neutral     1148\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c13cefc-f02b-4095-8557-228ac4efe3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data setelah menghapus duplikat:\n",
      "                                                   Tweet     Label\n",
      "0      warung ini dimiliki oleh pengusaha pabrik tahu...  positive\n",
      "1      mohon ulama lurus dan k212 mmbri hujjah partai...   neutral\n",
      "2      lokasi strategis di jalan sumatera bandung . t...  positive\n",
      "3      betapa bahagia nya diri ini saat unboxing pake...  positive\n",
      "4      duh . jadi mahasiswa jangan sombong dong . kas...  negative\n",
      "...                                                  ...       ...\n",
      "10993  f - demokrat dorong upaya kemandirian energi n...   neutral\n",
      "10994                                        tidak bosan  positive\n",
      "10996  enak rasa masakan nya apalagi kepiting yang me...  positive\n",
      "10998  pagi pagi di tol pasteur sudah macet parah , b...  negative\n",
      "10999  meskipun sering belanja ke yogya di riau junct...  positive\n",
      "\n",
      "[10933 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Menghapus duplikat pada kolom \"tweet\"\n",
    "df = df.drop_duplicates(subset='Tweet')\n",
    "\n",
    "# Memeriksa data setelah menghapus duplikat\n",
    "print(\"\\nData setelah menghapus duplikat:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ead9262c-0a8c-4765-a759-98fbfc7603b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing(sent):\n",
    "    # Mengubah kata menjadi huruf kecil semua dengan menggunakan fungsi lower()\n",
    "    string = sent.lower()\n",
    "\n",
    "    # Menghapus emoticon dan tanda baca menggunakan \"RegEx\" dengan script di bawah\n",
    "    string = re.sub(r'(?:\\@|http?\\://|https?\\://|www)\\S+', '', string) #menghapus https dan http\n",
    "    string = re.sub('<.*?>', ' ', string) #mengganti karakter html dengan tanda petik\n",
    "    string = re.sub('[^0-9a-zA-Z]+', ' ', string) #menghilangkan semua karakter yang bukan huruf atau angka dan menggantinya dengan spasi.\n",
    "    string = re.sub('\\n',' ',string) #mengganti line baru dengan spasi\n",
    "    string = re.sub(r':', ' ', string) #menggantikan karakter : dengan spasi \n",
    "    string = re.sub('gue','saya', string) # Mengganti kata \"gue\" dengan kata \"saya\"\n",
    "    string = re.sub(r'\\b[a-zA-Z]\\b', ' ', string) #menghapus single char\n",
    "    string = ' '.join(string.split()) #memisahkan dan menggabungkan kata\n",
    "    string = string.strip() #menghilangkan whitespace di awal dan di akhir teks\n",
    "    string = re.sub(r'pic.twitter.com.[\\w]+', '', string) #menghapus link picture\n",
    "    string = re.sub(r'\\buser\\b',' ', string) #menghapus kata 'user'\n",
    "    string = re.sub(r'\\brt\\b',' ', string) #menghapus awalan rt\n",
    "    string = re.sub('RT',' ', string) #menghapus RT simbol\n",
    "    string = re.sub(r'‚Ä¶', '', string) #menghapus simbol tidak perlu\n",
    "\n",
    "    # Lematisasi menggunakan Sastrawi\n",
    "    stemmer_factory = StemmerFactory()\n",
    "    stemmer = stemmer_factory.create_stemmer()\n",
    "    string = stemmer.stem(string)\n",
    "\n",
    "    # Menghapus stop words menggunakan Sastrawi\n",
    "    stopword_factory = StopWordRemoverFactory()\n",
    "    stopword_remover = stopword_factory.create_stop_word_remover()\n",
    "    string = stopword_remover.remove(string)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33b8e689-842b-40d5-a69d-c71730c0d3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet_Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>warung ini dimiliki oleh pengusaha pabrik tahu...</td>\n",
       "      <td>positive</td>\n",
       "      <td>warung milik usaha pabrik puluh tahun kenal pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mohon ulama lurus dan k212 mmbri hujjah partai...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ulama lurus k212 mmbri hujjah partai diwlh sua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lokasi strategis di jalan sumatera bandung . t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>lokasi strategis jalan sumatera bandung nyaman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>betapa bahagia nya diri ini saat unboxing pake...</td>\n",
       "      <td>positive</td>\n",
       "      <td>betapa bahagia unboxing paket barang bagus beli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>duh . jadi mahasiswa jangan sombong dong . kas...</td>\n",
       "      <td>negative</td>\n",
       "      <td>duh mahasiswa sombong kasih kartu kuning ajar ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet     Label  \\\n",
       "0  warung ini dimiliki oleh pengusaha pabrik tahu...  positive   \n",
       "1  mohon ulama lurus dan k212 mmbri hujjah partai...   neutral   \n",
       "2  lokasi strategis di jalan sumatera bandung . t...  positive   \n",
       "3  betapa bahagia nya diri ini saat unboxing pake...  positive   \n",
       "4  duh . jadi mahasiswa jangan sombong dong . kas...  negative   \n",
       "\n",
       "                                         Tweet_Clean  \n",
       "0  warung milik usaha pabrik puluh tahun kenal pu...  \n",
       "1  ulama lurus k212 mmbri hujjah partai diwlh sua...  \n",
       "2  lokasi strategis jalan sumatera bandung nyaman...  \n",
       "3    betapa bahagia unboxing paket barang bagus beli  \n",
       "4  duh mahasiswa sombong kasih kartu kuning ajar ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Tweet_Clean'] = df.Tweet.apply(cleansing)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b25f9bb-7ec5-493a-94a7-5f834518b8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 6383, Negative: 3412, Neutral: 1138\n",
      "Total data: 10933\n",
      "Total labels: 10933\n"
     ]
    }
   ],
   "source": [
    "# Group data tweet\n",
    "positive_tweet = df.loc[df['Label']=='positive'].Tweet.tolist()\n",
    "negative_tweet = df.loc[df['Label']=='negative'].Tweet.tolist()\n",
    "neutral_tweet = df.loc[df['Label']=='neutral'].Tweet.tolist()\n",
    "\n",
    "# Group data label\n",
    "positive_label = df.loc[df['Label']=='positive'].Label.tolist()\n",
    "negative_label = df.loc[df['Label']=='negative'].Label.tolist()\n",
    "neutral_label = df.loc[df['Label']=='neutral'].Label.tolist()\n",
    "\n",
    "total_data = positive_tweet + negative_tweet + neutral_tweet\n",
    "labels = positive_label + neutral_label + negative_label\n",
    "\n",
    "print(\"Positive: %s, Negative: %s, Neutral: %s\" % (len(positive_tweet), len(negative_tweet), len(neutral_tweet)))\n",
    "print(\"Total data: %s\" % len(total_data))\n",
    "print(\"Total labels: %s\" % len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54c034be-b070-4dfe-9233-f48d40861d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pickle has been created!\n",
      "x_pad_sequences.pickle has been created!\n",
      "tokenizer.pickle has been created!\n"
     ]
    }
   ],
   "source": [
    "max_features = 10000\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ', lower=True)\n",
    "tokenizer.fit_on_texts(total_data)\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"tokenizer.pickle has been created!\")\n",
    "\n",
    "X = tokenizer.texts_to_sequences(total_data)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "maxlen = max(len(x) for x in X)\n",
    "\n",
    "X = pad_sequences(X)\n",
    "with open('x_pad_sequences.pickle', 'wb') as handle:\n",
    "    pickle.dump(X, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"x_pad_sequences.pickle has been created!\")\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"tokenizer.pickle has been created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8743a328-df09-40cb-8468-b12b444807d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_labels.pickle has created!\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(labels)\n",
    "Y = Y.values\n",
    "\n",
    "with open('y_labels.pickle', 'wb') as handle:\n",
    "    pickle.dump(Y, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"y_labels.pickle has created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac025a52-80c0-4e8f-bbe3-cbe975df1744",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"x_pad_sequences.pickle\", 'rb')\n",
    "X = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"y_labels.pickle\", 'rb')\n",
    "Y = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57462995-4a7f-4820-9654-89613731ced3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 96, 100)           1000000   \n",
      "                                                                 \n",
      " simple_rnn_3 (SimpleRNN)    (None, 64)                10560     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1010755 (3.86 MB)\n",
      "Trainable params: 1010755 (3.86 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "875/875 [==============================] - 33s 33ms/step - loss: 0.5804 - accuracy: 0.7784 - val_loss: 0.5337 - val_accuracy: 0.7846\n",
      "Epoch 2/10\n",
      "875/875 [==============================] - 29s 33ms/step - loss: 0.3350 - accuracy: 0.8706 - val_loss: 0.6031 - val_accuracy: 0.7759\n",
      "Epoch 2: early stopping\n",
      "69/69 [==============================] - 1s 8ms/step\n",
      "Training - 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.69      0.69       681\n",
      "           1       0.29      0.10      0.15       235\n",
      "           2       0.85      0.95      0.90      1271\n",
      "\n",
      "    accuracy                           0.78      2187\n",
      "   macro avg       0.61      0.58      0.58      2187\n",
      "weighted avg       0.74      0.78      0.75      2187\n",
      "\n",
      "======================================================\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 96, 100)           1000000   \n",
      "                                                                 \n",
      " simple_rnn_4 (SimpleRNN)    (None, 64)                10560     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1010755 (3.86 MB)\n",
      "Trainable params: 1010755 (3.86 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "875/875 [==============================] - 31s 33ms/step - loss: 0.5778 - accuracy: 0.7796 - val_loss: 0.5168 - val_accuracy: 0.7965\n",
      "Epoch 2/10\n",
      "875/875 [==============================] - 29s 33ms/step - loss: 0.3276 - accuracy: 0.8769 - val_loss: 0.5834 - val_accuracy: 0.7801\n",
      "Epoch 2: early stopping\n",
      "69/69 [==============================] - 1s 8ms/step\n",
      "Training - 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.75      0.70       681\n",
      "           1       0.27      0.11      0.16       235\n",
      "           2       0.89      0.92      0.90      1271\n",
      "\n",
      "    accuracy                           0.78      2187\n",
      "   macro avg       0.61      0.60      0.59      2187\n",
      "weighted avg       0.75      0.78      0.76      2187\n",
      "\n",
      "======================================================\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 96, 100)           1000000   \n",
      "                                                                 \n",
      " simple_rnn_5 (SimpleRNN)    (None, 64)                10560     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1010755 (3.86 MB)\n",
      "Trainable params: 1010755 (3.86 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "875/875 [==============================] - 31s 33ms/step - loss: 0.6027 - accuracy: 0.7676 - val_loss: 0.5080 - val_accuracy: 0.7979\n",
      "Epoch 2/10\n",
      "875/875 [==============================] - 29s 33ms/step - loss: 0.3217 - accuracy: 0.8738 - val_loss: 0.6120 - val_accuracy: 0.7526\n",
      "Epoch 2: early stopping\n",
      "69/69 [==============================] - 1s 9ms/step\n",
      "Training - 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.61      0.64       681\n",
      "           1       0.26      0.14      0.18       235\n",
      "           2       0.83      0.94      0.88      1271\n",
      "\n",
      "    accuracy                           0.75      2187\n",
      "   macro avg       0.59      0.56      0.57      2187\n",
      "weighted avg       0.72      0.75      0.73      2187\n",
      "\n",
      "======================================================\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, 96, 100)           1000000   \n",
      "                                                                 \n",
      " simple_rnn_6 (SimpleRNN)    (None, 64)                10560     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1010755 (3.86 MB)\n",
      "Trainable params: 1010755 (3.86 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "875/875 [==============================] - 31s 33ms/step - loss: 0.6026 - accuracy: 0.7639 - val_loss: 0.5324 - val_accuracy: 0.7933\n",
      "Epoch 2/10\n",
      "875/875 [==============================] - 29s 33ms/step - loss: 0.3136 - accuracy: 0.8782 - val_loss: 0.6250 - val_accuracy: 0.7526\n",
      "Epoch 2: early stopping\n",
      "69/69 [==============================] - 1s 8ms/step\n",
      "Training - 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.68      0.66       681\n",
      "           1       0.24      0.17      0.20       235\n",
      "           2       0.88      0.90      0.89      1271\n",
      "\n",
      "    accuracy                           0.75      2187\n",
      "   macro avg       0.59      0.58      0.58      2187\n",
      "weighted avg       0.74      0.75      0.74      2187\n",
      "\n",
      "======================================================\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 96, 100)           1000000   \n",
      "                                                                 \n",
      " simple_rnn_7 (SimpleRNN)    (None, 64)                10560     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1010755 (3.86 MB)\n",
      "Trainable params: 1010755 (3.86 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "875/875 [==============================] - 32s 34ms/step - loss: 0.6674 - accuracy: 0.7302 - val_loss: 0.5731 - val_accuracy: 0.7856\n",
      "Epoch 2/10\n",
      "875/875 [==============================] - 32s 37ms/step - loss: 0.3846 - accuracy: 0.8499 - val_loss: 0.5884 - val_accuracy: 0.7833\n",
      "Epoch 2: early stopping\n",
      "69/69 [==============================] - 1s 10ms/step\n",
      "Training - 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.73      0.70       681\n",
      "           1       0.36      0.10      0.16       235\n",
      "           2       0.86      0.94      0.90      1271\n",
      "\n",
      "    accuracy                           0.78      2187\n",
      "   macro avg       0.63      0.59      0.59      2187\n",
      "weighted avg       0.75      0.78      0.76      2187\n",
      "\n",
      "======================================================\n",
      "\n",
      "\n",
      "\n",
      "Rata-rata Accuracy:  0.7689\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5,random_state=42,shuffle=True)\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "y = Y\n",
    "\n",
    "embed_dim = 100\n",
    "units = 64\n",
    "\n",
    "for iteration, data in enumerate(kf.split(X), start=1):\n",
    "    data_train   = X[data[0]]\n",
    "    target_train = y[data[0]]\n",
    "\n",
    "    data_test    = X[data[1]]\n",
    "    target_test  = y[data[1]]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embed_dim, input_length=X.shape[1]))\n",
    "    model.add(SimpleRNN(units, dropout=0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    sgd = optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    adam = optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=10, validation_data=(X_test, y_test), verbose=1, callbacks=[es])\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    y_pred = predictions\n",
    "\n",
    "    accuracy = accuracy_score(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "\n",
    "    print(\"Training -\", iteration)\n",
    "    print(classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1)))\n",
    "    print(\"======================================================\")\n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "average_accuracy = np.mean(accuracies)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print(\"Rata-rata Accuracy: \", round(average_accuracy,4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
